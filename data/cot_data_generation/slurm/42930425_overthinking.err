INFO:root:Saving results to output/model_llama-2-13b-chat-dataset_hotpot_qa-length_2-n_samples_20-n_examples_1-verify_False-cot_False-early_exit_trajectory_False-1707874059.jsonl
Downloading data:   0%|          | 0.00/301M [00:00<?, ?B/s]Downloading data:   1%|▏         | 4.19M/301M [00:00<00:28, 10.3MB/s]Downloading data:   4%|▍         | 12.6M/301M [00:00<00:12, 22.3MB/s]Downloading data:   7%|▋         | 21.0M/301M [00:00<00:09, 28.6MB/s]Downloading data:  10%|▉         | 29.4M/301M [00:01<00:09, 29.5MB/s]Downloading data:  13%|█▎        | 37.7M/301M [00:01<00:10, 24.4MB/s]Downloading data:  15%|█▌        | 46.1M/301M [00:02<00:16, 15.8MB/s]Downloading data:  18%|█▊        | 54.5M/301M [00:02<00:15, 15.8MB/s]Downloading data:  21%|██        | 62.9M/301M [00:03<00:13, 17.8MB/s]Downloading data:  24%|██▎       | 71.3M/301M [00:03<00:13, 17.3MB/s]Downloading data:  26%|██▋       | 79.7M/301M [00:04<00:12, 17.7MB/s]Downloading data:  29%|██▉       | 88.1M/301M [00:04<00:13, 15.3MB/s]Downloading data:  32%|███▏      | 96.5M/301M [00:05<00:11, 17.3MB/s]Downloading data:  35%|███▍      | 105M/301M [00:05<00:10, 18.3MB/s] Downloading data:  38%|███▊      | 113M/301M [00:06<00:10, 18.4MB/s]Downloading data:  40%|████      | 122M/301M [00:06<00:10, 16.7MB/s]Downloading data:  43%|████▎     | 130M/301M [00:07<00:08, 19.6MB/s]Downloading data:  46%|████▌     | 138M/301M [00:07<00:08, 18.8MB/s]Downloading data:  49%|████▉     | 147M/301M [00:07<00:06, 22.8MB/s]Downloading data:  52%|█████▏    | 155M/301M [00:08<00:07, 19.0MB/s]Downloading data:  54%|█████▍    | 164M/301M [00:08<00:06, 21.2MB/s]Downloading data:  57%|█████▋    | 172M/301M [00:09<00:06, 20.6MB/s]Downloading data:  60%|█████▉    | 180M/301M [00:09<00:04, 24.4MB/s]Downloading data:  63%|██████▎   | 189M/301M [00:09<00:04, 24.8MB/s]Downloading data:  66%|██████▌   | 197M/301M [00:10<00:06, 17.0MB/s]Downloading data:  68%|██████▊   | 206M/301M [00:11<00:06, 15.6MB/s]Downloading data:  71%|███████   | 214M/301M [00:11<00:04, 17.9MB/s]Downloading data:  74%|███████▍  | 222M/301M [00:11<00:04, 18.0MB/s]Downloading data:  77%|███████▋  | 231M/301M [00:12<00:03, 21.0MB/s]Downloading data:  79%|███████▉  | 239M/301M [00:12<00:02, 20.7MB/s]Downloading data:  82%|████████▏ | 247M/301M [00:12<00:02, 21.9MB/s]Downloading data:  85%|████████▌ | 256M/301M [00:13<00:01, 22.6MB/s]Downloading data:  88%|████████▊ | 264M/301M [00:13<00:01, 24.4MB/s]Downloading data:  91%|█████████ | 273M/301M [00:13<00:01, 23.2MB/s]Downloading data:  93%|█████████▎| 281M/301M [00:14<00:00, 19.8MB/s]Downloading data:  96%|█████████▌| 289M/301M [00:14<00:00, 19.0MB/s]Downloading data:  99%|█████████▉| 298M/301M [00:15<00:00, 18.9MB/s]Downloading data: 100%|██████████| 301M/301M [00:15<00:00, 19.6MB/s]
Downloading data:   0%|          | 0.00/31.1M [00:00<?, ?B/s]Downloading data:  13%|█▎        | 4.19M/31.1M [00:00<00:03, 8.35MB/s]Downloading data:  40%|████      | 12.6M/31.1M [00:00<00:01, 14.0MB/s]Downloading data:  67%|██████▋   | 21.0M/31.1M [00:01<00:00, 16.9MB/s]Downloading data:  94%|█████████▍| 29.4M/31.1M [00:01<00:00, 20.5MB/s]Downloading data: 100%|██████████| 31.1M/31.1M [00:01<00:00, 18.8MB/s]
Downloading data:   0%|          | 0.00/27.5M [00:00<?, ?B/s]Downloading data:  15%|█▌        | 4.19M/27.5M [00:00<00:02, 11.3MB/s]Downloading data:  46%|████▌     | 12.6M/27.5M [00:01<00:01, 10.0MB/s]Downloading data:  76%|███████▋  | 21.0M/27.5M [00:01<00:00, 11.9MB/s]Downloading data: 100%|██████████| 27.5M/27.5M [00:02<00:00, 14.5MB/s]Downloading data: 100%|██████████| 27.5M/27.5M [00:02<00:00, 13.1MB/s]
Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]Generating train split:  11%|█         | 10000/90447 [00:00<00:02, 34645.95 examples/s]Generating train split:  22%|██▏       | 20000/90447 [00:00<00:01, 36300.43 examples/s]Generating train split:  33%|███▎      | 30000/90447 [00:00<00:01, 36567.92 examples/s]Generating train split:  44%|████▍     | 40000/90447 [00:01<00:01, 36404.54 examples/s]Generating train split:  55%|█████▌    | 50000/90447 [00:01<00:01, 36434.70 examples/s]Generating train split:  66%|██████▋   | 60000/90447 [00:01<00:00, 36650.90 examples/s]Generating train split:  77%|███████▋  | 70000/90447 [00:01<00:00, 36265.72 examples/s]Generating train split:  88%|████████▊ | 80000/90447 [00:02<00:00, 36829.30 examples/s]Generating train split: 100%|██████████| 90447/90447 [00:02<00:00, 37713.53 examples/s]Generating train split: 100%|██████████| 90447/90447 [00:02<00:00, 36756.50 examples/s]
Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 7405/7405 [00:00<00:00, 37573.09 examples/s]Generating validation split: 100%|██████████| 7405/7405 [00:00<00:00, 36592.57 examples/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.21s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.05s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  6.64s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  7.03s/it]
Traceback (most recent call last):
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/main.py", line 172, in <module>
    main()
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/main.py", line 169, in main
    return run(args)
           ^^^^^^^^^
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/main.py", line 118, in run
    r["gpt_response"] = get_response(
                        ^^^^^^^^^^^^^
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/utils.py", line 30, in get_response
    return get_model_response(model, prompt, logit_bias=logit_bias, n_samples=n_samples, tokenizer=tokenizer, device=0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/utils.py", line 74, in get_model_response
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/generation/utils.py", line 1525, in generate
    return self.sample(
           ^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/generation/utils.py", line 2622, in sample
    outputs = self(
              ^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1070, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 812, in forward
    hidden_states = self.mlp(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 268, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 272.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 161.12 MiB is free. Including non-PyTorch memory, this process has 31.58 GiB memory in use. Of the allocated memory 29.50 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
INFO:root:Saving results to output/model_llama-2-13b-chat-dataset_hotpot_qa-length_2-n_samples_20-n_examples_1-verify_False-cot_True-early_exit_trajectory_False-1707874130.jsonl
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:15,  7.78s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:15<00:07,  7.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.30s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.67s/it]
Traceback (most recent call last):
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/main.py", line 172, in <module>
    main()
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/main.py", line 169, in main
    return run(args)
           ^^^^^^^^^
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/main.py", line 118, in run
    r["gpt_response"] = get_response(
                        ^^^^^^^^^^^^^
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/utils.py", line 30, in get_response
    return get_model_response(model, prompt, logit_bias=logit_bias, n_samples=n_samples, tokenizer=tokenizer, device=0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/utils.py", line 74, in get_model_response
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/generation/utils.py", line 1525, in generate
    return self.sample(
           ^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/generation/utils.py", line 2622, in sample
    outputs = self(
              ^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1070, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 812, in forward
    hidden_states = self.mlp(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 268, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 280.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 243.12 MiB is free. Including non-PyTorch memory, this process has 31.50 GiB memory in use. Of the allocated memory 29.43 GiB is allocated by PyTorch, and 1.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
