INFO:root:Saving results to output/model_llama-2-13b-chat-dataset_commonsense_qa-length_2-n_samples_20-n_examples_1-verify_False-cot_False-early_exit_trajectory_False-1707874059.jsonl
Downloading readme:   0%|          | 0.00/7.39k [00:00<?, ?B/s]Downloading readme: 100%|██████████| 7.39k/7.39k [00:00<00:00, 22.4MB/s]
Downloading data:   0%|          | 0.00/1.25M [00:00<?, ?B/s]Downloading data: 100%|██████████| 1.25M/1.25M [00:00<00:00, 6.07MB/s]Downloading data: 100%|██████████| 1.25M/1.25M [00:00<00:00, 5.99MB/s]
Downloading data:   0%|          | 0.00/160k [00:00<?, ?B/s]Downloading data: 100%|██████████| 160k/160k [00:00<00:00, 702kB/s]Downloading data: 100%|██████████| 160k/160k [00:00<00:00, 700kB/s]
Downloading data:   0%|          | 0.00/151k [00:00<?, ?B/s]Downloading data: 100%|██████████| 151k/151k [00:00<00:00, 590kB/s]Downloading data: 100%|██████████| 151k/151k [00:00<00:00, 588kB/s]
Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 9741/9741 [00:00<00:00, 531566.27 examples/s]
Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 1221/1221 [00:00<00:00, 417685.77 examples/s]
Generating test split:   0%|          | 0/1140 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1140/1140 [00:00<00:00, 408640.85 examples/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:18<00:37, 18.84s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:36<00:18, 18.10s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:47<00:00, 15.04s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:47<00:00, 15.94s/it]
INFO:root:Queried example 0
INFO:root:Example 0:
_________________

INFO:root:Prompt:

Question:
Where would someone likely bring an attache case?

Options:
A: overhead compartment
B: chair
C: hospital
D: in shirt
E: business meeting

INFO:root:Response: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'C', 'D', 'D']
INFO:root:True Solution: E
_________________
INFO:root:Results saved at output/model_llama-2-13b-chat-dataset_commonsense_qa-length_2-n_samples_20-n_examples_1-verify_False-cot_False-early_exit_trajectory_False-1707874059.jsonl
INFO:root:Saving results to output/model_llama-2-13b-chat-dataset_commonsense_qa-length_2-n_samples_20-n_examples_1-verify_False-cot_True-early_exit_trajectory_False-1707874125.jsonl
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:14,  7.03s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:14<00:07,  7.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:18<00:00,  5.78s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:18<00:00,  6.12s/it]
Traceback (most recent call last):
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/main.py", line 172, in <module>
    main()
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/main.py", line 169, in main
    return run(args)
           ^^^^^^^^^
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/main.py", line 118, in run
    r["gpt_response"] = get_response(
                        ^^^^^^^^^^^^^
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/utils.py", line 30, in get_response
    return get_model_response(model, prompt, logit_bias=logit_bias, n_samples=n_samples, tokenizer=tokenizer, device=0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jp5865/overthinking_the_truth/data/cot_data_generation/utils.py", line 74, in get_model_response
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/generation/utils.py", line 1525, in generate
    return self.sample(
           ^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/generation/utils.py", line 2622, in sample
    outputs = self(
              ^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1070, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 798, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 710, in forward
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/overthinking/lib/python3.11/site-packages/transformers/cache_utils.py", line 128, in update
    self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 31.12 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 30.01 GiB is allocated by PyTorch, and 1.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
